{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba74cd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:115: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:116: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:115: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:116: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "/var/folders/p9/wv3jwcsd19x8pf3r2kfnj0580000gn/T/ipykernel_74424/2634805998.py:115: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert (parameters['W' + str(l)].shape == layer_dims[l], layer_dims[l-1])\n",
      "/var/folders/p9/wv3jwcsd19x8pf3r2kfnj0580000gn/T/ipykernel_74424/2634805998.py:116: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert (parameters['W' + str(l)].shape == layer_dims[l], 1)\n"
     ]
    }
   ],
   "source": [
    "class ANN:\n",
    "    \n",
    "    def predict(self,X, y, parameters,active_func_h,active_func_o):\n",
    "        m = X.shape[1]\n",
    "        p = np.zeros((1,m), dtype = np.int64)\n",
    "        a3, caches = self.forward_propagation(X, parameters,active_func_h,active_func_o)\n",
    "        for i in range(0, a3.shape[1]):\n",
    "            if a3[0,i] > 0.5:\n",
    "                p[0,i] = 1\n",
    "            else:\n",
    "                p[0,i] = 0\n",
    "        #print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0]))))\n",
    "        a=str(np.mean((p[0,:] == y[0])))\n",
    "        return p,a\n",
    "\n",
    "    def update_parameters_with_gd(self,parameters, grads, learning_rate):\n",
    "        L = len(parameters) // 2 # number of layers in the neural networks\n",
    "        for l in range(L):\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        return parameters\n",
    "\n",
    "    def backward_propagation(self,X, Y, cache):\n",
    "        m = X.shape[1]\n",
    "        (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3) = cache\n",
    "\n",
    "        dz3 = 1./m * (a3 - Y)\n",
    "        dW3 = np.dot(dz3, a2.T)\n",
    "        db3 = np.sum(dz3, axis=1, keepdims = True)\n",
    "        \n",
    "        da2 = np.dot(W3.T, dz3)\n",
    "        dz2 = np.multiply(da2, np.int64(a2 > 0))\n",
    "        dW2 = np.dot(dz2, a1.T)\n",
    "        db2 = np.sum(dz2, axis=1, keepdims = True)\n",
    "        \n",
    "        da1 = np.dot(W2.T, dz2)\n",
    "        dz1 = np.multiply(da1, np.int64(a1 > 0))\n",
    "        dW1 = np.dot(dz1, X.T)\n",
    "        db1 = np.sum(dz1, axis=1, keepdims = True)\n",
    "        \n",
    "        gradients = {\"dz3\": dz3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"da2\": da2, \"dz2\": dz2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"da1\": da1, \"dz1\": dz1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "        return gradients\n",
    "\n",
    "    def compute_cost(self,a3, Y):\n",
    "        m = Y.shape[1]\n",
    "        logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n",
    "        cost = 1./m * np.sum(logprobs)\n",
    "        return cost\n",
    "\n",
    "    def relu(self,x):\n",
    "        s = np.maximum(0,x)\n",
    "        return s\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        s = 1/(1+np.exp(-x))\n",
    "        return s\n",
    "    \n",
    "    def leaky_relu(self,x, alpha=0.01):\n",
    "        return np.maximum(alpha*x, x)\n",
    "\n",
    "    def tanh(self,x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def softmax(self,x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "\n",
    "    def elu(self,z, alpha=1):\n",
    "        return np.where(z < 0, alpha * (np.exp(z) - 1), z)\n",
    "\n",
    "    def swish(self,z, beta=1):\n",
    "        return z / (1 + np.exp(-beta * z))\n",
    "\n",
    "    def softplus(self,z):\n",
    "        return np.log(1 + np.exp(z))\n",
    "\n",
    "    def softsign(self,z):\n",
    "        return z / (1 + np.abs(z))\n",
    "\n",
    "\n",
    "    def forward_propagation(self,X, parameters,active_func_h,active_func_o):\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        W3 = parameters[\"W3\"]\n",
    "        b3 = parameters[\"b3\"]\n",
    "        \n",
    "        if(active_func_h =='sigmoid'):\n",
    "            z1 = np.dot(W1, X) + b1\n",
    "            a1 = self.sigmoid(z1)\n",
    "            z2 = np.dot(W2, a1) + b2\n",
    "            a2 = self.sigmoid(z2)\n",
    "            \n",
    "        elif(active_func_h=='relu'):\n",
    "            z1 = np.dot(W1, X) + b1\n",
    "            a1 = self.relu(z1)\n",
    "            z2 = np.dot(W2, a1) + b2\n",
    "            a2 = self.relu(z2)\n",
    "            \n",
    "        elif(active_func_h=='leaky_relu'):\n",
    "            z1 = np.dot(W1, X) + b1\n",
    "            a1 = self.leaky_relu(z1)\n",
    "            z2 = np.dot(W2, a1) + b2\n",
    "            a2 = self.leaky_relu(z2)\n",
    "            \n",
    "        elif(active_func_h=='tanh'):\n",
    "            z1 = np.dot(W1, X) + b1\n",
    "            a1 = self.tanh(z1)\n",
    "            z2 = np.dot(W2, a1) + b2\n",
    "            a2 = self.tanh(z2)\n",
    "            \n",
    "        elif(active_func_h=='elu'):\n",
    "            z1 = np.dot(W1, X) + b1\n",
    "            a1 = self.elu(z1)\n",
    "            z2 = np.dot(W2, a1) + b2\n",
    "            a2 = self.elu(z2)\n",
    "            \n",
    "        elif(active_func_h=='softplus'):\n",
    "            z1 = np.dot(W1, X) + b1\n",
    "            a1 = self.softplus(z1)\n",
    "            z2 = np.dot(W2, a1) + b2\n",
    "            a2 = self.softplus(z2)\n",
    "        \n",
    "        if(active_func_o=='sigmoid'):\n",
    "            z3 = np.dot(W3, a2) + b3\n",
    "            a3 = self.sigmoid(z3)\n",
    "        \n",
    "        elif(active_func_o=='softmax'):\n",
    "            z3 = np.dot(W3, a2) + b3\n",
    "            a3 = self.softmax(z3)\n",
    "        \n",
    "    \n",
    "        cache = (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3)\n",
    "    \n",
    "        return a3, cache\n",
    "\n",
    "    def random_mini_batches(self,X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \n",
    "        np.random.seed(seed)            \n",
    "        m = X.shape[1]                 \n",
    "        mini_batches = []\n",
    "        \n",
    "        permutation = list(np.random.permutation(m))   \n",
    "        shuffled_X = X[:, permutation]\n",
    "        shuffled_Y = Y[permutation].values.reshape((1, m))\n",
    "        #shuffled_Y = Y[permutation].reshape((1, m))\n",
    "        \n",
    "\n",
    "        num_complete_minibatches = math.floor(m/mini_batch_size) \n",
    "        for k in range(0, num_complete_minibatches):\n",
    "            mini_batch_X = shuffled_X[:, k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "            mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "    \n",
    "        if m % mini_batch_size != 0:\n",
    "            mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : ]\n",
    "            mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : ]\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "    \n",
    "        return mini_batches\n",
    "\n",
    "    def initialize_parameters(self,layer_dims):\n",
    "        np.random.seed(3)\n",
    "        parameters = {}\n",
    "        L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "        for l in range(1, L):\n",
    "            parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*  np.sqrt(2 / layer_dims[l-1])\n",
    "            parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "            assert (parameters['W' + str(l)].shape == layer_dims[l], layer_dims[l-1])\n",
    "            assert (parameters['W' + str(l)].shape == layer_dims[l], 1)\n",
    "        \n",
    "        return parameters\n",
    "    # GRADED FUNCTION: update_parameters_with_momentum\n",
    "\n",
    "    def update_parameters_with_momentum(self,parameters, grads, v, beta, learning_rate):\n",
    "  \n",
    "        L = len(parameters) // 2\n",
    "        for l in range(L):\n",
    "            v[\"dW\" + str(l+1)] = beta * v[\"dW\" + str(l+1)] + (1 - beta) * grads[\"dW\" + str(l+1)]\n",
    "            v[\"db\" + str(l+1)] = beta * v[\"db\" + str(l+1)] + (1 - beta) * grads[\"db\" + str(l+1)]\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v[\"dW\" + str(l+1)]\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v[\"db\" + str(l+1)]\n",
    "        \n",
    "        return parameters, v\n",
    "\n",
    "    def initialize_velocity(self,parameters):\n",
    "        L = len(parameters) // 2 \n",
    "        v = {}\n",
    "        for l in range(L):\n",
    "            v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\"+str(l+1)].shape)\n",
    "            v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\"+str(l+1)].shape)\n",
    "        return v\n",
    "    \n",
    "    def update_parameters_with_adam(self,parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "        L = len(parameters) // 2                 \n",
    "        v_corrected = {}                         \n",
    "        s_corrected = {}                         \n",
    "    \n",
    "        for l in range(L):\n",
    "            v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads[\"dW\" + str(l+1)]\n",
    "            v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads[\"db\" + str(l+1)]\n",
    "            v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - beta1)\n",
    "            v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - beta1)\n",
    "            s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * (grads[\"dW\" + str(l+1)] * grads[\"dW\" + str(l+1)])\n",
    "            s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * (grads[\"db\" + str(l+1)] * grads[\"db\" + str(l+1)])\n",
    "            s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - beta2)\n",
    "            s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - beta2)\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"dW\" + str(l+1)] / (np.sqrt(s_corrected[\"dW\" + str(l+1)]) + epsilon)\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / (np.sqrt(s_corrected[\"db\" + str(l+1)]) + epsilon)\n",
    "\n",
    "        return parameters, v, s\n",
    "\n",
    "    def initialize_adam(self,parameters):\n",
    "        L = len(parameters) // 2 \n",
    "        v = {}\n",
    "        s = {}\n",
    "        for l in range(L):\n",
    "            v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "            v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "            s[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "            s[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)    \n",
    "        return v, s\n",
    "    \n",
    "    def model(self,X, Y, layers_dims, optimizer, learning_rate = 0.001, mini_batch_size =64, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 1000, active_func_h=None, active_func_o=None, print_cost = True):\n",
    "        acc=0\n",
    "        L = len(layers_dims)            \n",
    "        costs = []\n",
    "        accuracy=[]\n",
    "        t = 0                            \n",
    "        seed = 10                        \n",
    "        parameters = self.initialize_parameters(layers_dims)\n",
    "        \n",
    "        if optimizer == \"gd\":\n",
    "            pass \n",
    "        elif optimizer == \"momentum\":\n",
    "            v = self.initialize_velocity(parameters)\n",
    "        elif optimizer == \"adam\":\n",
    "            v, s = self.initialize_adam(parameters)\n",
    "    \n",
    "        for i in range(num_epochs):\n",
    "            seed = seed + 1\n",
    "            minibatches = self.random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                a3, caches = self.forward_propagation(minibatch_X, parameters,active_func_h,active_func_o)\n",
    "                cost = self.compute_cost(a3, minibatch_Y)\n",
    "                grads = self.backward_propagation(minibatch_X, minibatch_Y, caches)\n",
    "                if optimizer == \"gd\":\n",
    "                    parameters = self.update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "                elif optimizer == \"momentum\":\n",
    "                    parameters, v = self.update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "                elif optimizer == \"adam\":\n",
    "                    t = t + 1 \n",
    "                    parameters, v, s = self.update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "            \n",
    "            if print_cost and i % 100 == 0:\n",
    "                p,a=self.predict(minibatch_X, minibatch_Y, parameters,active_func_h,active_func_o)\n",
    "                print (\"Cost after epoch %i: %f\" %(i, cost),'Accuracy: ',a)\n",
    "            if print_cost and i % 1 == 0:\n",
    "                costs.append(cost)\n",
    "                p,a=self.predict(minibatch_X, minibatch_Y, parameters,active_func_h,active_func_o)\n",
    "                accuracy.append(float(a))\n",
    "            if print_cost and i == (num_epochs-1):\n",
    "                p,a=self.predict(minibatch_X, minibatch_Y, parameters,active_func_h,active_func_o)\n",
    "                acc=a\n",
    "                \n",
    "        \n",
    "        plt.plot(costs)\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('epochs (per 100)')\n",
    "        plt.title(\"Cost @ Learning rate = \" + str(learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "        plt.plot(accuracy)\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epochs (per 100)')\n",
    "        plt.title(\"Accuracy @ Learning rate = \" + str(learning_rate))\n",
    "        plt.show()\n",
    "        print ('Accuracy: ',acc)\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20060f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191df48e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
